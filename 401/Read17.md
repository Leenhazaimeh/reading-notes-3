# web scraping

- web scraping , web harvesting or web data extraction is the automatic process of collecting data from world wide web using a bot and filling it into a database or a spreadsheet.

- web scraping started earlier after world wide web invented in 1989 with 4 years.

- your role as a web crawler or scrapper creator is to prevent the websites from blocking you.

- usually websites can identify a spider(web scrapper) by:
1. high download speed and very fast browsing.
2. following a special behavior which normal people don't do
3. so many request in a short period of time from the same IP address

- when you got blocked from scrapping , you will likely see 404 not found , 403 forbidden, 429 run timeout , too many requests , and many more.

- you should also know that web scrapping have some legal consequences in some countries such as USA , EU , India 

- Methods to prevent web scraping
The administrator of a website can use various measures to stop or slow a bot. Some techniques include:

1. Blocking an IP address either manually or based on criteria such as geolocation and DNSRBL. This will also block all browsing from that address.
Disabling any web service API that the website's system might expose.
2. Bots sometimes declare who they are (using user agent strings) and can be blocked on that basis using robots.txt.
3. Bots can be blocked by monitoring excess traffic
4. Bots can sometimes be blocked with tools to verify that it is a real person accessing the site, like a CAPTCHA.
5. Locating bots with a honeypot or other method to identify the IP addresses of automated crawlers.