# Dunder Methods
- Definition: In Python, special methods are a set of predefined methods you can use to enrich your classes. They are easy to recognize because they start and end with double underscores, for example __init__ or __str__

- Dunder methods let you emulate the behavior of built-in types. For example, to get the length of a string you can call len('string'). But an empty class definition doesn’t support this behavior out of the box:

        class NoLenSupport: pass
        obj = NoLenSupport() len(obj)
        TypeError: "object of type 'NoLenSupport' has no len()"

    To fix this, you can add a len dunder method to your class:
        class LenSupport: def __len__(self): return 42
        obj = LenSupport() len(obj)
        42

### Enriching a Simple Account Class
- Object Initialization: init
Right upon starting my class we need a special method. To construct account objects from the Account class I need a constructor which in Python is the dunder init

- Object Representation: str, repr
It’s common practice in Python to provide a string representation of your object for the consumer of your class

- Operator Overloading for Comparing Accounts: eq, lt
To not have to implement all of the comparison dunder methods, you use the functools.total_ordering decorator which allows you to take a shortcut, only implementing eq and lt

- Operator Overloading for Merging Accounts: add
In Python, everything is an object. We are completely fine adding two integers or two strings with the + (plus) operator

- Callable Python Objects: call
You can make an object callable like a regular function by adding the call dunder method.

# What is probability?
At the most basic level, probability seeks to answer the question, “What is the chance of an event to be happened?” An event is some outcome of interest. To calculate the chance of an event happening, we also need to consider all the other events that can occur.

### From statistics to probability
Our data will be generated by flipping a coin 10 times and counting how many times we get heads. We will call a set of 10 coin tosses a trial. Our data point will be the number of heads we observe. We may not get the “ideal” 5 heads, but we won’t worry too much since one trial is only one data point. If we perform many, many trials, we expect the average number of heads over all of our trials to approach the 50%. The code below simulates 10, 100, 1000, and 1000000 trials, and then calculates the average proportion of heads observed. Our process is summarized in the image below as well.

        import random
        def coin_trial():
            heads = 0
            for i in range(100):
                if random.random() <= 0.5:
                heads +=1
            return heads
        def simulate(n):
        trials = []
        for i in range(n):
            trials.append(coin_trial())
        return(sum(trials)/n)
        simulate(10)
        5.4
        simulate(100)
        4.83
        simulate(1000)
        5.055
        simulate(1000000)
        4.999781

### The Data and the Distribution:
The most important qualities to notice about the normal distribution is its symmetry and its shape. We’ve been calling it a distribution, but what exactly is being distributed? It depends on the context. In probability, the normal distribution is a particular distribution of the probability across all of the events. The x-axis takes on the values of events we want to know the probability of. The y-axis is the probability associated with each event, from 0 to 1.

- In a probability context, the high point in a normal distribution represents the event with the highest probability of occurring. As you get farther away from this event on either side, the probability drops rapidly, forming that familiar bell-shape. The high point in a statistical context actually represents the mean. As in probability, as you get farther from the mean, you rapidly drop off in frequency.

- We’ll bring in the wine data and then separate out the scores of some wines of interest to us. To bring back in the data, we need the following code:

    import csv
    with open("wine-data.csv", "r", encoding="latin-1") as f: wines = list(csv.reader(f))

- Three Sigma Rule:
The Three Sigma rule, also known as the empirical rule or 68-95-99.7 rule, is an expression of how many of our observations fall within a certain distance of the mean.

- The Three Sigma rule dictates that given a normal distribution

- 68% of your observations will fall between one standard deviation of the mean. 95% will fall within two, and 99.7% will fall within three.

### Z-score:
The Z-score is a simple calculation that answers the question, “Given a data point, how many standard deviations is it away from the mean?”

- the Z-score doesn’t provide much information to you. It gains the most value when compared against a Z-table, which tabulates the cumulative probability of a standard normal distribution up until a given Z-score.

- Conclusion
We started with descriptive statistics and then connected them to probability. From probability, we developed a way to quantatively show if two groups come from the same distribution. In this case, we compared two wine recommendations and found that they most likely do not come from the same score distribution.